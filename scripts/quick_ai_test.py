#!/usr/bin/env python3
"""
å¿«é€ŸAIæµ‹è¯• - ä¸ä¾èµ–å®Œæ•´çš„åº”ç”¨é…ç½®

ç›´æ¥æµ‹è¯•Ollamaå’ŒAIè°ƒç”¨åŠŸèƒ½
"""

import asyncio
import sys
import httpx
import json
import os


async def test_ollama_connection():
    """æµ‹è¯•Ollamaè¿æ¥"""
    print("\n" + "="*60)
    print("æµ‹è¯• 1: OllamaæœåŠ¡è¿æ¥")
    print("="*60)

    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get("http://localhost:11434/api/tags")
            if response.status_code == 200:
                data = response.json()
                models = data.get("models", [])
                print(f"âœ… Ollamaè¿è¡Œæ­£å¸¸")
                print(f"   ç«¯å£: 11434")
                print(f"   å¯ç”¨æ¨¡å‹æ•°: {len(models)}")
                if models:
                    print(f"\n   å¯ç”¨æ¨¡å‹:")
                    for model in models:
                        size_gb = model['size'] / (1024**3)
                        print(f"     â€¢ {model['name']:30s} ({size_gb:.1f} GB)")
                return True, models
            else:
                print(f"âŒ Ollamaå“åº”å¼‚å¸¸: {response.status_code}")
                return False, []
    except Exception as e:
        print(f"âŒ Ollamaè¿æ¥å¤±è´¥")
        print(f"   é”™è¯¯: {e}")
        print(f"\n   ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print(f"      1. å®‰è£…Ollama: curl -fsSL https://ollama.com/install.sh | sh")
        print(f"      2. å¯åŠ¨æœåŠ¡: ollama serve")
        print(f"      3. ä¸‹è½½æ¨¡å‹: ollama pull qwen2.5:7b")
        return False, []


async def test_ollama_chat(model_name="qwen2:latest"):
    """æµ‹è¯•Ollamaå¯¹è¯åŠŸèƒ½"""
    print("\n" + "="*60)
    print(f"æµ‹è¯• 2: Ollamaå¯¹è¯æµ‹è¯• (æ¨¡å‹: {model_name})")
    print("="*60)

    try:
        print(f"å‘é€æµ‹è¯•æ¶ˆæ¯...")

        request_data = {
            "model": model_name,
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ•èµ„åˆ†æåŠ©æ‰‹ã€‚"},
                {"role": "user", "content": "ç”¨ä¸€å¥è¯ä»‹ç»è´µå·èŒ…å°"}
            ],
            "stream": False,
            "options": {
                "temperature": 0.7,
                "num_predict": 100
            }
        }

        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                "http://localhost:11434/api/chat",
                json=request_data
            )

            if response.status_code == 200:
                data = response.json()
                ai_reply = data.get("message", {}).get("content", "")

                print(f"âœ… AIè°ƒç”¨æˆåŠŸ")
                print(f"\n   AIå›å¤:")
                print(f"   {ai_reply}")
                print(f"\n   æ¨¡å‹: {data.get('model')}")
                print(f"   è€—æ—¶: ~{data.get('total_duration', 0) / 1e9:.2f}ç§’")

                return True, ai_reply
            else:
                print(f"âŒ AIè°ƒç”¨å¤±è´¥: {response.status_code}")
                print(f"   å“åº”: {response.text[:200]}")
                return False, None

    except Exception as e:
        print(f"âŒ AIè°ƒç”¨å¼‚å¸¸")
        print(f"   é”™è¯¯: {e}")
        return False, None


async def test_stock_analysis_prompt():
    """æµ‹è¯•è‚¡ç¥¨åˆ†æPrompt"""
    print("\n" + "="*60)
    print("æµ‹è¯• 3: è‚¡ç¥¨åˆ†æPrompt")
    print("="*60)

    prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæŠ•èµ„åˆ†æå¸ˆã€‚è¯·å¯¹ä»¥ä¸‹è‚¡ç¥¨è¿›è¡Œç»¼åˆåˆ†æï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ç»“æœã€‚

**è‚¡ç¥¨ä¿¡æ¯**:
- è‚¡ç¥¨ä»£ç : 600519
- è‚¡ç¥¨åç§°: è´µå·èŒ…å°

**åˆ†æè¦æ±‚**:
è¯·æä¾›ä»¥ä¸‹ç»´åº¦çš„åˆ†æ:
1. åŸºæœ¬é¢åˆ†æ (fundamental_score: 0-100åˆ†)
2. æŠ€æœ¯é¢åˆ†æ (technical_score: 0-100åˆ†)
3. ä¼°å€¼åˆ†æ (valuation_score: 0-100åˆ†)
4. ç»¼åˆè¯„åˆ† (overall_score: 0-100åˆ†)
5. æŠ•èµ„å»ºè®® (ai_suggestion: å­—ç¬¦ä¸²)
6. æŠ•èµ„ç†ç”± (ai_reasons: å­—ç¬¦ä¸²æ•°ç»„)
7. ç½®ä¿¡åº¦ (confidence_level: 0-100)

**è¾“å‡ºæ ¼å¼** (å¿…é¡»ä¸¥æ ¼éµå®ˆJSONæ ¼å¼):
```json
{
  "ai_score": {
    "fundamental_score": 75,
    "technical_score": 68,
    "valuation_score": 82,
    "overall_score": 75
  },
  "ai_suggestion": "å»ºè®®æŒæœ‰ï¼Œä¸­é•¿æœŸçœ‹å¥½",
  "ai_strategy": {
    "target_price": 1800.0,
    "recommended_position": 15.0,
    "risk_level": "medium",
    "holding_period": "6-12ä¸ªæœˆ",
    "stop_loss_price": 1500.0
  },
  "ai_reasons": ["ç†ç”±1", "ç†ç”±2", "ç†ç”±3"],
  "confidence_level": 78.5
}
```

è¯·åªè¿”å›JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚
"""

    print(f"âœ… Promptæ„å»ºæˆåŠŸ")
    print(f"   Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
    print(f"\n   Prompté¢„è§ˆ:")
    print(f"   {prompt[:200]}...")

    return True, prompt


async def test_full_stock_analysis(model_name="qwen2:latest"):
    """å®Œæ•´çš„è‚¡ç¥¨åˆ†ææµ‹è¯•"""
    print("\n" + "="*60)
    print(f"æµ‹è¯• 4: å®Œæ•´è‚¡ç¥¨åˆ†ææµç¨‹")
    print("="*60)

    _, prompt = await test_stock_analysis_prompt()

    print(f"\næ­£åœ¨è°ƒç”¨AIè¿›è¡Œè‚¡ç¥¨åˆ†æ...")
    print(f"(è¿™å¯èƒ½éœ€è¦5-10ç§’)")

    try:
        request_data = {
            "model": model_name,
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ•èµ„åˆ†æå¸ˆï¼Œç²¾é€šè‚¡ç¥¨åˆ†æã€‚"},
                {"role": "user", "content": prompt}
            ],
            "stream": False,
            "options": {
                "temperature": 0.7,
                "num_predict": 1000
            }
        }

        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                "http://localhost:11434/api/chat",
                json=request_data
            )

            if response.status_code == 200:
                data = response.json()
                ai_reply = data.get("message", {}).get("content", "")

                print(f"\nâœ… è‚¡ç¥¨åˆ†ææˆåŠŸ")
                print(f"\n   AIåˆ†æç»“æœ:")
                print(f"   {ai_reply[:500]}...")

                # å°è¯•è§£æJSON
                try:
                    # æå–JSON
                    if "```json" in ai_reply:
                        start = ai_reply.find("```json") + 7
                        end = ai_reply.find("```", start)
                        json_str = ai_reply[start:end].strip()
                    elif "```" in ai_reply:
                        start = ai_reply.find("```") + 3
                        end = ai_reply.find("```", start)
                        json_str = ai_reply[start:end].strip()
                    elif "{" in ai_reply:
                        start = ai_reply.find("{")
                        end = ai_reply.rfind("}") + 1
                        json_str = ai_reply[start:end]
                    else:
                        json_str = ai_reply

                    result = json.loads(json_str)
                    print(f"\n   âœ… JSONè§£ææˆåŠŸ")
                    print(f"   ç»¼åˆè¯„åˆ†: {result.get('ai_score', {}).get('overall_score', 'N/A')}")
                    print(f"   æŠ•èµ„å»ºè®®: {result.get('ai_suggestion', 'N/A')}")
                    print(f"   ç½®ä¿¡åº¦: {result.get('confidence_level', 'N/A')}")

                    return True, result

                except Exception as e:
                    print(f"\n   âš ï¸  JSONè§£æå¤±è´¥: {e}")
                    print(f"   ä½†AIè°ƒç”¨æˆåŠŸï¼Œå¯èƒ½éœ€è¦ä¼˜åŒ–Prompt")
                    return True, None

            else:
                print(f"âŒ åˆ†æå¤±è´¥: {response.status_code}")
                return False, None

    except Exception as e:
        print(f"âŒ åˆ†æå¼‚å¸¸: {e}")
        return False, None


async def check_deepseek_config():
    """æ£€æŸ¥DeepSeeké…ç½®"""
    print("\n" + "="*60)
    print("æ£€æŸ¥ DeepSeek APIé…ç½®")
    print("="*60)

    # å°è¯•ä»ç¯å¢ƒå˜é‡æˆ–.envæ–‡ä»¶è¯»å–
    deepseek_key = os.getenv("DEEPSEEK_API_KEY")

    if deepseek_key:
        print(f"âœ… DeepSeek API Keyå·²é…ç½®")
        print(f"   Key: {deepseek_key[:10]}...{deepseek_key[-4:]}")
        return True
    else:
        print(f"â„¹ï¸  DeepSeek API Keyæœªé…ç½®")
        print(f"   å¦‚éœ€ä½¿ç”¨DeepSeek APIï¼Œè¯·åœ¨ backend/.env ä¸­æ·»åŠ :")
        print(f"   DEEPSEEK_API_KEY=sk-xxxxxxxxxxxxxxxx")
        return False


async def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("ğŸš€ AIåŠŸèƒ½å¿«é€Ÿæµ‹è¯•")
    print("="*60)

    # æµ‹è¯•Ollamaè¿æ¥
    ollama_ok, models = await test_ollama_connection()

    if not ollama_ok:
        print("\n" + "="*60)
        print("âš ï¸  æµ‹è¯•ä¸­æ­¢")
        print("="*60)
        print("Ollamaæœªè¿è¡Œï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
        print("\nè¯·å…ˆ:")
        print("1. å®‰è£…å¹¶å¯åŠ¨Ollama")
        print("2. ä¸‹è½½æ¨¡å‹: ollama pull qwen2.5:7b")
        return 1

    # é€‰æ‹©æ¨¡å‹
    model_name = None
    for model in models:
        name = model['name']
        if 'qwen2' in name.lower() or 'deepseek' in name.lower():
            if 'vl' not in name.lower():  # è·³è¿‡è§†è§‰æ¨¡å‹
                model_name = name
                break

    if not model_name and models:
        model_name = models[0]['name']

    print(f"\nä½¿ç”¨æ¨¡å‹: {model_name}")

    # æµ‹è¯•å¯¹è¯
    chat_ok, _ = await test_ollama_chat(model_name)

    if chat_ok:
        # æµ‹è¯•å®Œæ•´è‚¡ç¥¨åˆ†æ
        await test_full_stock_analysis(model_name)

    # æ£€æŸ¥DeepSeeké…ç½®
    await check_deepseek_config()

    # æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("="*60)

    if ollama_ok and chat_ok:
        print("âœ… Ollamaè¿æ¥æ­£å¸¸")
        print("âœ… AIå¯¹è¯åŠŸèƒ½æ­£å¸¸")
        print("âœ… å¯ä»¥å¼€å§‹ä½¿ç”¨AIåŠŸèƒ½")
        print("\nä¸‹ä¸€æ­¥:")
        print("1. å¯åŠ¨åç«¯: ./scripts/dev.sh")
        print("2. è¿è¡Œå®Œæ•´æµ‹è¯•: python scripts/test_ai_features.py")
        return 0
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        print("è¯·æ£€æŸ¥é…ç½®")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
